{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8fe7bf-3707-4978-a486-a961280436b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f047db-511c-4a54-9d3e-72675fc4f303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature  Ice Cream Profits\n",
      "0           39              13.17\n",
      "1           40              11.88\n",
      "2           41              18.82\n",
      "3           42              18.65\n",
      "4           43              17.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('IceCreamSales-temperatures.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8122127c-fa89-4451-88c4-c860be2b638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de características y variable objetivo\n",
    "X = data.drop('Ice Cream Profits', axis=1) #La independiente\n",
    "y = data['Ice Cream Profits'] #La que quiero predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6c8eb6-82ff-44c7-9f21-0d63fffdebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945df17a-5c55-4177-ae8d-0bf56088b3c2",
   "metadata": {},
   "source": [
    "## Métricas de Evaluación de Modelos\n",
    "\n",
    "Las métricas de evaluación como precisión, recall, y F1-score son esenciales para entender diferentes aspectos del rendimiento del modelo. Aquí demostramos cómo calcular estas métricas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f0065f-0b26-448d-9236-aee29dcc39f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calcular métricas\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:421\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of y is not strictly positive which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis necessary for Poisson regression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         )\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 421\u001b[0m y, expanded_class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_y_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m DOUBLE \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m y\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous:\n\u001b[0;32m    424\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(y, dtype\u001b[38;5;241m=\u001b[39mDOUBLE)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:831\u001b[0m, in \u001b[0;36mForestClassifier._validate_y_class_weight\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_y_class_weight\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m--> 831\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(y)\n\u001b[0;32m    834\u001b[0m     expanded_class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\multiclass.py:221\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    213\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m ]:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113efba-9160-4412-b82b-15b51d353074",
   "metadata": {},
   "source": [
    "Estas métricas indican un alto desempeño del modelo en el conjunto de prueba, reflejando su capacidad para predecir correctamente tanto las clases positivas como las negativas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d89c27-37e2-4b94-9706-5b7fc07112fd",
   "metadata": {},
   "source": [
    "### Explicación de las Métricas de Evaluación y sus Resultados\n",
    "\n",
    "En el contexto de la validación y evaluación de modelos de machine learning, es fundamental entender qué mide cada métrica y cómo interpretar los resultados obtenidos. A continuación, se presentan las definiciones y el análisis de las métricas de evaluación: Accuracy, Recall, Precision, F1 Score y ROC AUC Score, junto con sus resultados específicos.\n",
    "\n",
    "#### Accuracy (Precisión)\n",
    "**Definición:**\n",
    "La precisión es la proporción de predicciones correctas realizadas por el modelo sobre el total de predicciones. Es una métrica global que indica qué tan bien está funcionando el modelo en general.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- TP: Verdaderos Positivos\n",
    "- TN: Verdaderos Negativos\n",
    "- FP: Falsos Positivos\n",
    "- FN: Falsos Negativos\n",
    "\n",
    "**Resultado: 0.96**\n",
    "- **Interpretación:** El modelo predice correctamente el 96% de los casos. Esto indica un buen rendimiento general, pero no distingue entre las diferentes clases de errores (falsos positivos y falsos negativos).\n",
    "\n",
    "#### Recall (Sensibilidad o Tasa de Verdaderos Positivos)\n",
    "**Definición:**\n",
    "El recall mide la capacidad del modelo para identificar todas las instancias positivas. Es especialmente importante en contextos donde no detectar una instancia positiva tiene un alto costo.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.99**\n",
    "- **Interpretación:** El modelo identifica correctamente el 99% de las instancias positivas. Este alto valor de recall es crucial en situaciones como el diagnóstico médico, donde es importante no pasar por alto ningún caso positivo.\n",
    "\n",
    "#### Precision (Precisión o Valor Predictivo Positivo)\n",
    "**Definición:**\n",
    "La precisión mide la exactitud de las predicciones positivas del modelo. Indica la proporción de verdaderos positivos sobre todas las predicciones positivas.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.96**\n",
    "- **Interpretación:** El 96% de las instancias que el modelo predice como positivas son realmente positivas. Una alta precisión es importante en casos donde los falsos positivos son costosos.\n",
    "\n",
    "#### F1 Score\n",
    "**Definición:**\n",
    "El F1 Score es la media armónica de la precisión y el recall. Proporciona una única métrica que equilibra la precisión y el recall, especialmente útil cuando se necesita un balance entre ambas.\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.97**\n",
    "- **Interpretación:** El F1 Score de 0.97 indica un equilibrio casi perfecto entre precisión y recall, mostrando que el modelo es muy bueno tanto en identificar instancias positivas como en evitar falsos positivos.\n",
    "\n",
    "#### ROC AUC Score (Área bajo la curva ROC)\n",
    "**Definición:**\n",
    "El ROC AUC Score mide la capacidad del modelo para distinguir entre clases. La curva ROC traza la tasa de verdaderos positivos frente a la tasa de falsos positivos en varios umbrales de clasificación.\n",
    "\n",
    "$$\n",
    "\\text{AUC} = \\int_{\\text{ROC}} \\text{d(ROC)}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.96**\n",
    "- **Interpretación:** Un ROC AUC Score de 0.96 significa que el modelo tiene una excelente capacidad para distinguir entre clases positivas y negativas. Cuanto más cercano a 1, mejor es la discriminación.\n",
    "\n",
    "### Conclusión\n",
    "Las métricas obtenidas reflejan un modelo que no solo es preciso en sus predicciones generales (accuracy), sino que también es extremadamente eficaz en identificar correctamente las instancias positivas (recall) y en asegurar que las predicciones positivas sean verdaderamente positivas (precision). El F1 Score y el ROC AUC Score complementan esta evaluación al mostrar un excelente balance y capacidad de discriminación, respectivamente. Estos resultados indican que el modelo está bien ajustado y es robusto para su implementación en un entorno de producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6df62f-6ad1-43b4-a63e-5c4d86a54f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
